#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Mar 23 17:45:20 2020

@author: sahanaasokan


Evaluating Classification Models Performance

False Positive = less dangerous (type 1 error)
False Negative = most dangerous (type 2 error)

Confusion Matrix

y actual vs y^ predicted

1 0 vs 0 1

combinations 

use an earthquake example
1 and 1 will happen and did happen
0 and 0 wont happen and didnt happen
0 and 1  we are predicting it happened but it didnt happened type 1 error
earthquake didnt happen but we predicted it did.
1 and 0 we are predicting it doesnt happen but it actually happened. type 1 error
earthquake did happen but we predicted it didnt. 
type 2 error


it depends on the scenario.

accuracy rate = correct/total
error rate = wrong/total


Accuracy Paradox
when accuracy is too high may not necessarily be the most accurate prediction.


CAP Curve

















"""


